{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "610bNp_4SQma",
    "outputId": "3792d912-5b76-4731-f640-369b9930f6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading https://files.pythonhosted.org/packages/00/92/a05b76a692ac08d470ae5c23873cf1c9a041532f1ee065e74b374f218306/contractions-0.0.25-py2.py3-none-any.whl\n",
      "Collecting textsearch\n",
      "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
      "Collecting Unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 7.3MB/s \n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 14.1MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81703 sha256=b0b59ffcdb093858e1c16dd82debf048944a8bc08ad29456fa619c3bedf8feec\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
      "Successfully built pyahocorasick\n",
      "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
      "Successfully installed Unidecode-1.1.1 contractions-0.0.25 pyahocorasick-1.4.0 textsearch-0.0.17\n",
      "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (0.0.17)\n",
      "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.1.1)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.4.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install textsearch\n",
    "!pip install tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMFvLtEHRaFM"
   },
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgphrYufRaFR",
    "outputId": "6144528b-6658-4dd4-b95f-23a8068c9119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('movie_metadata.csv')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "5nkEEGExRaFc",
    "outputId": "bf0b9586-ee96-4b4f-bbb8-82c43b6f1ed8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V1NWGhcRaFi"
   },
   "source": [
    "# Build Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7JP10IEYRaFj"
   },
   "outputs": [],
   "source": [
    "# build train and test datasets\n",
    "reviews = dataset['review'].values\n",
    "sentiments = dataset['sentiment'].values\n",
    "\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i45AFxXNRaFn"
   },
   "source": [
    "# Text Wrangling & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oHZ0lEGNRaFo"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    doc = doc.lower()\n",
    "    doc = remove_accented_chars(doc)\n",
    "    doc = contractions.fix(doc)\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    doc = doc.strip()  \n",
    "    norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CO3ESug2RaFr",
    "outputId": "1801f1a3-92ad-4fb8-b0de-a10425096b81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35000/35000 [00:16<00:00, 2125.14it/s]\n",
      "100%|██████████| 15000/15000 [00:07<00:00, 2120.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 253 ms, total: 23.6 s\n",
      "Wall time: 23.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucDv8n50RaFu"
   },
   "source": [
    "# Traditional Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOZ7Rn0jRaFv"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JD8q5QoERaFw",
    "outputId": "35caedc1-d9e0-4128-96de-2dd863516c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.8 s, sys: 847 ms, total: 48.6 s\n",
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
    "\n",
    "\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=5, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(norm_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSvqpHRYRaFz",
    "outputId": "ac3f2a49-7122-44da-fb05-e31d3f9d787a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 7.88 ms, total: 11.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(norm_test_reviews)\n",
    "tv_test_features = tv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfQPYw8PRaF2",
    "outputId": "ba2a5d38-784b-47c9-aa1d-6beda0c6a81b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (35000, 194919)  Test features shape: (15000, 194919)\n",
      "TFIDF model:> Train features shape: (35000, 194919)  Test features shape: (15000, 194919)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aUUsxMrRaF7"
   },
   "source": [
    "## Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPA5UtYFRaF8"
   },
   "source": [
    "###  Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGHQErnMRaF9",
    "outputId": "f762ebb2-bcbf-45b8-9419-ef6727befb97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 48.9 s, total: 2min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression model on BOW features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate model\n",
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, solver='lbfgs', random_state=42)\n",
    "\n",
    "# train model\n",
    "lr.fit(cv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "lr_bow_predictions = lr.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "wqeRyayrRaGA",
    "outputId": "712051be-81fc-4595-f53c-f43fb73584ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.90      0.90      7490\n",
      "    positive       0.90      0.91      0.90      7510\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6756</td>\n",
       "      <td>734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>707</td>\n",
       "      <td>6803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6756       734\n",
       "positive       707      6803"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, lr_bow_predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, lr_bow_predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQEAz6O6RaGC",
    "outputId": "033b2582-6dd2-4c4b-d3d3-5e2e050f0913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.29 s, sys: 2.16 s, total: 5.44 s\n",
      "Wall time: 2.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression model on TF-IDF features\n",
    "\n",
    "# train model\n",
    "lr.fit(tv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "lr_tfidf_predictions = lr.predict(tv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "HNCfZnUORaGE",
    "outputId": "c28cf597-00cc-461a-a857-331e2127e458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      7490\n",
      "    positive       0.90      0.91      0.90      7510\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6688</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>666</td>\n",
       "      <td>6844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6688       802\n",
       "positive       666      6844"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, lr_tfidf_predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, lr_tfidf_predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoOoEAiXRaGH"
   },
   "source": [
    "###  Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YzaSSOpYRaGH",
    "outputId": "996f1a01-b16a-4076-80f3-c1d113026497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 15s, sys: 248 ms, total: 4min 15s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Random Forest model on BOW features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate model\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "\n",
    "# train model\n",
    "rf.fit(cv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "rf_bow_predictions = rf.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "617Kuv7_RaGJ",
    "outputId": "6323fcce-ad4e-49fd-907b-726868597aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.86      0.86      7490\n",
      "    positive       0.86      0.86      0.86      7510\n",
      "\n",
      "    accuracy                           0.86     15000\n",
      "   macro avg       0.86      0.86      0.86     15000\n",
      "weighted avg       0.86      0.86      0.86     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6410</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1060</td>\n",
       "      <td>6450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6410      1080\n",
       "positive      1060      6450"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, rf_bow_predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, rf_bow_predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdvmBOrPRaGM",
    "outputId": "03462da2-a880-4cfb-924d-35ba6987c4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 46s, sys: 286 ms, total: 3min 47s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Random Forest model on TF-IDF features\n",
    "\n",
    "# train model\n",
    "rf.fit(tv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "rf_tfidf_predictions = rf.predict(tv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "8hBOMh6uRaGP",
    "outputId": "d905c7b7-9243-4196-843f-d766c61741d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.86      0.85      7490\n",
      "    positive       0.86      0.84      0.85      7510\n",
      "\n",
      "    accuracy                           0.85     15000\n",
      "   macro avg       0.85      0.85      0.85     15000\n",
      "weighted avg       0.85      0.85      0.85     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6439</td>\n",
       "      <td>1051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1216</td>\n",
       "      <td>6294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6439      1051\n",
       "positive      1216      6294"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, rf_tfidf_predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, rf_tfidf_predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoqZhMQFRaGS"
   },
   "source": [
    "#  Supervised Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "QZw6LYNHRaGT"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiZbcv_gRaGZ"
   },
   "source": [
    "## Prediction class label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JnhC4rWaRaGb"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "# tokenize train reviews & encode train labels\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                       for text in norm_train_reviews]\n",
    "y_train = le.fit_transform(train_sentiments)\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                       for text in norm_test_reviews]\n",
    "y_test = le.fit_transform(test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ogDRDh4RaGg",
    "outputId": "5eef14b0-71f1-47dc-c564-17a3148f1da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
      "Sample test label transformation:\n",
      "----------------------------------- \n",
      "Actual Labels: ['negative' 'positive' 'negative'] \n",
      "Encoded Labels: [0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# print class label encoding map and encoded labels\n",
    "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print('Sample test label transformation:\\n'+'-'*35,\n",
    "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdexbrYXRaGk"
   },
   "source": [
    "## Feature Engineering with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "G5S0u0BbiN2a"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9kfCw6LRaGl",
    "outputId": "6573dda7-baf7-457d-9149-73313e619a3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-02 07:46:37,502 : INFO : collecting all words and their counts\n",
      "2020-12-02 07:46:37,503 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-12-02 07:46:37,988 : INFO : PROGRESS: at sentence #10000, processed 2294821 words, keeping 82417 word types\n",
      "2020-12-02 07:46:38,477 : INFO : PROGRESS: at sentence #20000, processed 4590857 words, keeping 124831 word types\n",
      "2020-12-02 07:46:38,952 : INFO : PROGRESS: at sentence #30000, processed 6884108 words, keeping 159824 word types\n",
      "2020-12-02 07:46:39,204 : INFO : collected 176257 word types from a corpus of 8034983 raw words and 35000 sentences\n",
      "2020-12-02 07:46:39,205 : INFO : Loading a fresh vocabulary\n",
      "2020-12-02 07:46:39,318 : INFO : effective_min_count=10 retains 24662 unique words (13% of original 176257, drops 151595)\n",
      "2020-12-02 07:46:39,319 : INFO : effective_min_count=10 leaves 7762730 word corpus (96% of original 8034983, drops 272253)\n",
      "2020-12-02 07:46:39,405 : INFO : deleting the raw counts dictionary of 176257 items\n",
      "2020-12-02 07:46:39,411 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2020-12-02 07:46:39,414 : INFO : downsampling leaves estimated 5720709 word corpus (73.7% of prior 7762730)\n",
      "2020-12-02 07:46:39,496 : INFO : estimated required memory for 24662 words and 300 dimensions: 71519800 bytes\n",
      "2020-12-02 07:46:39,497 : INFO : resetting layer weights\n",
      "2020-12-02 07:46:44,292 : INFO : training model with 4 workers on 24662 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=150\n",
      "2020-12-02 07:46:45,345 : INFO : EPOCH 1 - PROGRESS: at 1.32% examples, 73600 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:46,458 : INFO : EPOCH 1 - PROGRESS: at 2.82% examples, 74846 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:47,476 : INFO : EPOCH 1 - PROGRESS: at 4.49% examples, 79385 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:48,476 : INFO : EPOCH 1 - PROGRESS: at 6.00% examples, 80485 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:49,516 : INFO : EPOCH 1 - PROGRESS: at 7.56% examples, 81927 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:50,538 : INFO : EPOCH 1 - PROGRESS: at 8.96% examples, 81905 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:51,544 : INFO : EPOCH 1 - PROGRESS: at 10.56% examples, 83078 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:52,591 : INFO : EPOCH 1 - PROGRESS: at 11.93% examples, 81860 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:53,623 : INFO : EPOCH 1 - PROGRESS: at 13.37% examples, 81761 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:54,637 : INFO : EPOCH 1 - PROGRESS: at 14.77% examples, 81877 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:55,644 : INFO : EPOCH 1 - PROGRESS: at 16.14% examples, 81949 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:56,650 : INFO : EPOCH 1 - PROGRESS: at 17.35% examples, 81538 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:46:57,669 : INFO : EPOCH 1 - PROGRESS: at 19.07% examples, 82122 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:58,758 : INFO : EPOCH 1 - PROGRESS: at 20.65% examples, 82231 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:46:59,904 : INFO : EPOCH 1 - PROGRESS: at 22.30% examples, 82058 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:00,906 : INFO : EPOCH 1 - PROGRESS: at 23.89% examples, 82556 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:01,919 : INFO : EPOCH 1 - PROGRESS: at 25.30% examples, 82582 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:02,923 : INFO : EPOCH 1 - PROGRESS: at 26.76% examples, 82575 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:03,929 : INFO : EPOCH 1 - PROGRESS: at 28.34% examples, 82644 words/s, in_qsize 7, out_qsize 1\n",
      "2020-12-02 07:47:04,929 : INFO : EPOCH 1 - PROGRESS: at 29.73% examples, 82647 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:06,053 : INFO : EPOCH 1 - PROGRESS: at 31.39% examples, 82879 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:07,205 : INFO : EPOCH 1 - PROGRESS: at 32.92% examples, 82620 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:08,223 : INFO : EPOCH 1 - PROGRESS: at 34.38% examples, 82618 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:09,264 : INFO : EPOCH 1 - PROGRESS: at 36.05% examples, 82810 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:47:10,312 : INFO : EPOCH 1 - PROGRESS: at 37.52% examples, 82708 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:11,417 : INFO : EPOCH 1 - PROGRESS: at 39.07% examples, 82681 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:12,460 : INFO : EPOCH 1 - PROGRESS: at 40.66% examples, 82837 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:13,514 : INFO : EPOCH 1 - PROGRESS: at 42.18% examples, 82738 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:14,518 : INFO : EPOCH 1 - PROGRESS: at 43.79% examples, 83005 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:15,600 : INFO : EPOCH 1 - PROGRESS: at 45.37% examples, 83023 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:16,622 : INFO : EPOCH 1 - PROGRESS: at 46.81% examples, 83025 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:17,656 : INFO : EPOCH 1 - PROGRESS: at 48.31% examples, 82995 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:18,690 : INFO : EPOCH 1 - PROGRESS: at 49.80% examples, 82917 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:19,712 : INFO : EPOCH 1 - PROGRESS: at 51.32% examples, 82911 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:20,712 : INFO : EPOCH 1 - PROGRESS: at 52.81% examples, 82949 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:21,732 : INFO : EPOCH 1 - PROGRESS: at 54.30% examples, 82940 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:22,791 : INFO : EPOCH 1 - PROGRESS: at 55.92% examples, 83018 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:47:23,912 : INFO : EPOCH 1 - PROGRESS: at 57.41% examples, 82960 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:24,927 : INFO : EPOCH 1 - PROGRESS: at 58.82% examples, 82951 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:26,006 : INFO : EPOCH 1 - PROGRESS: at 60.41% examples, 82996 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:27,018 : INFO : EPOCH 1 - PROGRESS: at 61.90% examples, 82995 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:28,047 : INFO : EPOCH 1 - PROGRESS: at 63.51% examples, 83114 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:29,049 : INFO : EPOCH 1 - PROGRESS: at 64.80% examples, 82974 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:30,085 : INFO : EPOCH 1 - PROGRESS: at 66.26% examples, 82929 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:31,242 : INFO : EPOCH 1 - PROGRESS: at 67.99% examples, 82976 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:32,244 : INFO : EPOCH 1 - PROGRESS: at 69.48% examples, 82989 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:47:33,279 : INFO : EPOCH 1 - PROGRESS: at 71.08% examples, 83103 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:34,284 : INFO : EPOCH 1 - PROGRESS: at 72.57% examples, 83120 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:35,511 : INFO : EPOCH 1 - PROGRESS: at 74.20% examples, 83034 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:36,529 : INFO : EPOCH 1 - PROGRESS: at 75.87% examples, 83137 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:37,572 : INFO : EPOCH 1 - PROGRESS: at 77.51% examples, 83222 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:38,594 : INFO : EPOCH 1 - PROGRESS: at 78.99% examples, 83207 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:47:39,624 : INFO : EPOCH 1 - PROGRESS: at 80.40% examples, 83190 words/s, in_qsize 7, out_qsize 1\n",
      "2020-12-02 07:47:40,668 : INFO : EPOCH 1 - PROGRESS: at 82.02% examples, 83245 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:41,698 : INFO : EPOCH 1 - PROGRESS: at 83.59% examples, 83212 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:42,715 : INFO : EPOCH 1 - PROGRESS: at 84.89% examples, 83089 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:43,720 : INFO : EPOCH 1 - PROGRESS: at 86.34% examples, 83109 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:47:44,739 : INFO : EPOCH 1 - PROGRESS: at 87.75% examples, 83101 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:45,789 : INFO : EPOCH 1 - PROGRESS: at 89.29% examples, 83052 words/s, in_qsize 7, out_qsize 1\n",
      "2020-12-02 07:47:46,819 : INFO : EPOCH 1 - PROGRESS: at 90.64% examples, 83029 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:47,842 : INFO : EPOCH 1 - PROGRESS: at 92.09% examples, 83009 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:48,892 : INFO : EPOCH 1 - PROGRESS: at 93.69% examples, 83062 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:49,922 : INFO : EPOCH 1 - PROGRESS: at 95.33% examples, 83149 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:50,928 : INFO : EPOCH 1 - PROGRESS: at 96.91% examples, 83148 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:51,996 : INFO : EPOCH 1 - PROGRESS: at 98.46% examples, 83177 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:52,885 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-02 07:47:52,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-02 07:47:52,965 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-02 07:47:52,974 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-02 07:47:52,975 : INFO : EPOCH - 1 : training on 8034983 raw words (5720155 effective words) took 68.7s, 83292 effective words/s\n",
      "2020-12-02 07:47:54,097 : INFO : EPOCH 2 - PROGRESS: at 1.32% examples, 69162 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:55,129 : INFO : EPOCH 2 - PROGRESS: at 2.94% examples, 78565 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:56,190 : INFO : EPOCH 2 - PROGRESS: at 4.61% examples, 80747 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:57,357 : INFO : EPOCH 2 - PROGRESS: at 6.34% examples, 81582 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:58,391 : INFO : EPOCH 2 - PROGRESS: at 7.81% examples, 81574 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:47:59,442 : INFO : EPOCH 2 - PROGRESS: at 9.32% examples, 82275 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:00,469 : INFO : EPOCH 2 - PROGRESS: at 10.81% examples, 82232 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:01,471 : INFO : EPOCH 2 - PROGRESS: at 12.45% examples, 83202 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:02,560 : INFO : EPOCH 2 - PROGRESS: at 13.69% examples, 81722 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:03,609 : INFO : EPOCH 2 - PROGRESS: at 15.34% examples, 82879 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:04,643 : INFO : EPOCH 2 - PROGRESS: at 16.69% examples, 82737 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:05,694 : INFO : EPOCH 2 - PROGRESS: at 18.19% examples, 82518 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:06,775 : INFO : EPOCH 2 - PROGRESS: at 19.81% examples, 82631 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:07,897 : INFO : EPOCH 2 - PROGRESS: at 21.42% examples, 82556 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:08,952 : INFO : EPOCH 2 - PROGRESS: at 23.03% examples, 82772 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:09,958 : INFO : EPOCH 2 - PROGRESS: at 24.49% examples, 82850 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:10,979 : INFO : EPOCH 2 - PROGRESS: at 26.02% examples, 83129 words/s, in_qsize 7, out_qsize 1\n",
      "2020-12-02 07:48:12,098 : INFO : EPOCH 2 - PROGRESS: at 27.69% examples, 83031 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:13,101 : INFO : EPOCH 2 - PROGRESS: at 29.16% examples, 83033 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:48:14,146 : INFO : EPOCH 2 - PROGRESS: at 30.66% examples, 83215 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:15,159 : INFO : EPOCH 2 - PROGRESS: at 32.04% examples, 83123 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:16,198 : INFO : EPOCH 2 - PROGRESS: at 33.65% examples, 83309 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:17,203 : INFO : EPOCH 2 - PROGRESS: at 35.18% examples, 83316 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:18,231 : INFO : EPOCH 2 - PROGRESS: at 36.65% examples, 83254 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:19,258 : INFO : EPOCH 2 - PROGRESS: at 38.09% examples, 83199 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:20,327 : INFO : EPOCH 2 - PROGRESS: at 39.68% examples, 83278 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:21,539 : INFO : EPOCH 2 - PROGRESS: at 41.41% examples, 83169 words/s, in_qsize 7, out_qsize 1\n",
      "2020-12-02 07:48:22,551 : INFO : EPOCH 2 - PROGRESS: at 42.93% examples, 83154 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:23,573 : INFO : EPOCH 2 - PROGRESS: at 44.36% examples, 83110 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:24,574 : INFO : EPOCH 2 - PROGRESS: at 45.93% examples, 83370 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:25,601 : INFO : EPOCH 2 - PROGRESS: at 47.59% examples, 83567 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:26,670 : INFO : EPOCH 2 - PROGRESS: at 49.15% examples, 83599 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:27,673 : INFO : EPOCH 2 - PROGRESS: at 50.84% examples, 83816 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:28,713 : INFO : EPOCH 2 - PROGRESS: at 52.32% examples, 83750 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:29,738 : INFO : EPOCH 2 - PROGRESS: at 53.77% examples, 83721 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:30,734 : INFO : EPOCH 2 - PROGRESS: at 55.29% examples, 83711 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:31,975 : INFO : EPOCH 2 - PROGRESS: at 56.83% examples, 83394 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:33,010 : INFO : EPOCH 2 - PROGRESS: at 58.38% examples, 83496 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:34,061 : INFO : EPOCH 2 - PROGRESS: at 59.83% examples, 83407 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:35,075 : INFO : EPOCH 2 - PROGRESS: at 61.41% examples, 83563 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:36,122 : INFO : EPOCH 2 - PROGRESS: at 62.89% examples, 83475 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:37,140 : INFO : EPOCH 2 - PROGRESS: at 64.32% examples, 83456 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:38,160 : INFO : EPOCH 2 - PROGRESS: at 65.75% examples, 83426 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:48:39,172 : INFO : EPOCH 2 - PROGRESS: at 67.25% examples, 83425 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:40,184 : INFO : EPOCH 2 - PROGRESS: at 68.90% examples, 83571 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:41,210 : INFO : EPOCH 2 - PROGRESS: at 70.33% examples, 83534 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:42,225 : INFO : EPOCH 2 - PROGRESS: at 71.83% examples, 83511 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:43,253 : INFO : EPOCH 2 - PROGRESS: at 73.25% examples, 83483 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:44,296 : INFO : EPOCH 2 - PROGRESS: at 74.72% examples, 83414 words/s, in_qsize 5, out_qsize 2\n",
      "2020-12-02 07:48:45,592 : INFO : EPOCH 2 - PROGRESS: at 76.75% examples, 83472 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:46,611 : INFO : EPOCH 2 - PROGRESS: at 78.36% examples, 83584 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:47,645 : INFO : EPOCH 2 - PROGRESS: at 79.79% examples, 83548 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:48,685 : INFO : EPOCH 2 - PROGRESS: at 81.44% examples, 83609 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:49,732 : INFO : EPOCH 2 - PROGRESS: at 82.93% examples, 83550 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:50,831 : INFO : EPOCH 2 - PROGRESS: at 84.58% examples, 83539 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:48:51,859 : INFO : EPOCH 2 - PROGRESS: at 86.02% examples, 83519 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:52,868 : INFO : EPOCH 2 - PROGRESS: at 87.41% examples, 83518 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:53,883 : INFO : EPOCH 2 - PROGRESS: at 88.88% examples, 83512 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:48:54,898 : INFO : EPOCH 2 - PROGRESS: at 90.30% examples, 83504 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:55,927 : INFO : EPOCH 2 - PROGRESS: at 91.75% examples, 83461 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:57,007 : INFO : EPOCH 2 - PROGRESS: at 93.43% examples, 83576 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:58,018 : INFO : EPOCH 2 - PROGRESS: at 95.01% examples, 83578 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:48:59,044 : INFO : EPOCH 2 - PROGRESS: at 96.65% examples, 83657 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:00,059 : INFO : EPOCH 2 - PROGRESS: at 98.10% examples, 83633 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:01,106 : INFO : EPOCH 2 - PROGRESS: at 99.51% examples, 83577 words/s, in_qsize 4, out_qsize 0\n",
      "2020-12-02 07:49:01,163 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-02 07:49:01,237 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-02 07:49:01,254 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-02 07:49:01,267 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-02 07:49:01,267 : INFO : EPOCH - 2 : training on 8034983 raw words (5720197 effective words) took 68.3s, 83768 effective words/s\n",
      "2020-12-02 07:49:02,434 : INFO : EPOCH 3 - PROGRESS: at 1.32% examples, 66569 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:03,457 : INFO : EPOCH 3 - PROGRESS: at 2.94% examples, 77222 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:04,463 : INFO : EPOCH 3 - PROGRESS: at 4.49% examples, 79001 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:05,467 : INFO : EPOCH 3 - PROGRESS: at 6.13% examples, 81802 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:06,500 : INFO : EPOCH 3 - PROGRESS: at 7.45% examples, 80566 words/s, in_qsize 6, out_qsize 2\n",
      "2020-12-02 07:49:07,591 : INFO : EPOCH 3 - PROGRESS: at 8.96% examples, 80784 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:49:08,595 : INFO : EPOCH 3 - PROGRESS: at 10.56% examples, 82119 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:09,629 : INFO : EPOCH 3 - PROGRESS: at 12.08% examples, 81977 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:10,642 : INFO : EPOCH 3 - PROGRESS: at 13.50% examples, 81989 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:49:11,652 : INFO : EPOCH 3 - PROGRESS: at 14.90% examples, 82146 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:12,681 : INFO : EPOCH 3 - PROGRESS: at 16.25% examples, 82024 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:13,711 : INFO : EPOCH 3 - PROGRESS: at 17.65% examples, 81996 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:14,728 : INFO : EPOCH 3 - PROGRESS: at 19.17% examples, 82046 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:49:15,768 : INFO : EPOCH 3 - PROGRESS: at 20.75% examples, 82446 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:16,909 : INFO : EPOCH 3 - PROGRESS: at 22.52% examples, 82713 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:18,054 : INFO : EPOCH 3 - PROGRESS: at 24.15% examples, 82491 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:19,079 : INFO : EPOCH 3 - PROGRESS: at 25.67% examples, 82799 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:20,163 : INFO : EPOCH 3 - PROGRESS: at 27.41% examples, 83231 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:21,279 : INFO : EPOCH 3 - PROGRESS: at 29.04% examples, 83095 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:22,355 : INFO : EPOCH 3 - PROGRESS: at 30.41% examples, 82821 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:23,421 : INFO : EPOCH 3 - PROGRESS: at 31.94% examples, 82879 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:24,562 : INFO : EPOCH 3 - PROGRESS: at 33.52% examples, 82713 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:25,643 : INFO : EPOCH 3 - PROGRESS: at 35.18% examples, 82770 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:26,687 : INFO : EPOCH 3 - PROGRESS: at 36.65% examples, 82674 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:27,703 : INFO : EPOCH 3 - PROGRESS: at 38.21% examples, 82951 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:28,750 : INFO : EPOCH 3 - PROGRESS: at 39.68% examples, 82851 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:29,757 : INFO : EPOCH 3 - PROGRESS: at 41.15% examples, 82854 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:30,925 : INFO : EPOCH 3 - PROGRESS: at 42.93% examples, 82903 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:31,942 : INFO : EPOCH 3 - PROGRESS: at 44.35% examples, 82897 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:32,978 : INFO : EPOCH 3 - PROGRESS: at 45.93% examples, 83054 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:34,071 : INFO : EPOCH 3 - PROGRESS: at 47.59% examples, 83089 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:35,190 : INFO : EPOCH 3 - PROGRESS: at 49.15% examples, 83018 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:36,274 : INFO : EPOCH 3 - PROGRESS: at 50.84% examples, 83058 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:37,287 : INFO : EPOCH 3 - PROGRESS: at 52.30% examples, 83075 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:38,307 : INFO : EPOCH 3 - PROGRESS: at 53.77% examples, 83069 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:39,345 : INFO : EPOCH 3 - PROGRESS: at 55.57% examples, 83356 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:40,384 : INFO : EPOCH 3 - PROGRESS: at 56.96% examples, 83299 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:41,394 : INFO : EPOCH 3 - PROGRESS: at 58.38% examples, 83277 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:49:42,562 : INFO : EPOCH 3 - PROGRESS: at 59.94% examples, 83131 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:43,760 : INFO : EPOCH 3 - PROGRESS: at 61.69% examples, 83095 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:44,767 : INFO : EPOCH 3 - PROGRESS: at 63.28% examples, 83266 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:45,773 : INFO : EPOCH 3 - PROGRESS: at 64.70% examples, 83267 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:46,900 : INFO : EPOCH 3 - PROGRESS: at 66.25% examples, 83200 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:47,946 : INFO : EPOCH 3 - PROGRESS: at 67.86% examples, 83298 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:48,983 : INFO : EPOCH 3 - PROGRESS: at 69.47% examples, 83388 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:50,015 : INFO : EPOCH 3 - PROGRESS: at 71.08% examples, 83495 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:51,093 : INFO : EPOCH 3 - PROGRESS: at 72.57% examples, 83381 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:52,120 : INFO : EPOCH 3 - PROGRESS: at 73.94% examples, 83343 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:53,185 : INFO : EPOCH 3 - PROGRESS: at 75.63% examples, 83365 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:54,269 : INFO : EPOCH 3 - PROGRESS: at 77.27% examples, 83382 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:55,290 : INFO : EPOCH 3 - PROGRESS: at 78.73% examples, 83365 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:56,323 : INFO : EPOCH 3 - PROGRESS: at 80.14% examples, 83331 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:57,361 : INFO : EPOCH 3 - PROGRESS: at 81.79% examples, 83411 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:58,515 : INFO : EPOCH 3 - PROGRESS: at 83.47% examples, 83313 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:49:59,570 : INFO : EPOCH 3 - PROGRESS: at 84.89% examples, 83256 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:00,581 : INFO : EPOCH 3 - PROGRESS: at 86.45% examples, 83382 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:01,585 : INFO : EPOCH 3 - PROGRESS: at 87.87% examples, 83392 words/s, in_qsize 6, out_qsize 0\n",
      "2020-12-02 07:50:02,646 : INFO : EPOCH 3 - PROGRESS: at 89.38% examples, 83325 words/s, in_qsize 8, out_qsize 2\n",
      "2020-12-02 07:50:03,699 : INFO : EPOCH 3 - PROGRESS: at 90.77% examples, 83263 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:50:04,714 : INFO : EPOCH 3 - PROGRESS: at 92.35% examples, 83363 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:05,729 : INFO : EPOCH 3 - PROGRESS: at 93.96% examples, 83456 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:06,731 : INFO : EPOCH 3 - PROGRESS: at 95.49% examples, 83468 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:07,833 : INFO : EPOCH 3 - PROGRESS: at 97.17% examples, 83449 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:08,847 : INFO : EPOCH 3 - PROGRESS: at 98.57% examples, 83433 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:09,637 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-02 07:50:09,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-02 07:50:09,668 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-02 07:50:09,700 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-02 07:50:09,701 : INFO : EPOCH - 3 : training on 8034983 raw words (5720358 effective words) took 68.4s, 83598 effective words/s\n",
      "2020-12-02 07:50:10,831 : INFO : EPOCH 4 - PROGRESS: at 1.31% examples, 69130 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:11,868 : INFO : EPOCH 4 - PROGRESS: at 2.94% examples, 78202 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:12,918 : INFO : EPOCH 4 - PROGRESS: at 4.61% examples, 80741 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:13,974 : INFO : EPOCH 4 - PROGRESS: at 6.24% examples, 82081 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:15,111 : INFO : EPOCH 4 - PROGRESS: at 7.81% examples, 81709 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:16,124 : INFO : EPOCH 4 - PROGRESS: at 9.20% examples, 81775 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:17,151 : INFO : EPOCH 4 - PROGRESS: at 10.69% examples, 81806 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:18,179 : INFO : EPOCH 4 - PROGRESS: at 12.31% examples, 82614 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:50:19,232 : INFO : EPOCH 4 - PROGRESS: at 13.72% examples, 82225 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:50:20,248 : INFO : EPOCH 4 - PROGRESS: at 15.12% examples, 82239 words/s, in_qsize 7, out_qsize 1\n",
      "2020-12-02 07:50:21,295 : INFO : EPOCH 4 - PROGRESS: at 16.47% examples, 82026 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:22,323 : INFO : EPOCH 4 - PROGRESS: at 17.90% examples, 82082 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:23,341 : INFO : EPOCH 4 - PROGRESS: at 19.52% examples, 82546 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:24,349 : INFO : EPOCH 4 - PROGRESS: at 21.04% examples, 82639 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:25,364 : INFO : EPOCH 4 - PROGRESS: at 22.53% examples, 82629 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:26,365 : INFO : EPOCH 4 - PROGRESS: at 24.04% examples, 82712 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:27,378 : INFO : EPOCH 4 - PROGRESS: at 25.43% examples, 82715 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:28,499 : INFO : EPOCH 4 - PROGRESS: at 27.01% examples, 82574 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:29,601 : INFO : EPOCH 4 - PROGRESS: at 28.70% examples, 82578 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:30,611 : INFO : EPOCH 4 - PROGRESS: at 30.06% examples, 82576 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:31,634 : INFO : EPOCH 4 - PROGRESS: at 31.48% examples, 82526 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:32,751 : INFO : EPOCH 4 - PROGRESS: at 33.05% examples, 82442 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:33,839 : INFO : EPOCH 4 - PROGRESS: at 34.68% examples, 82498 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:34,846 : INFO : EPOCH 4 - PROGRESS: at 36.17% examples, 82512 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:35,882 : INFO : EPOCH 4 - PROGRESS: at 37.75% examples, 82735 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:36,902 : INFO : EPOCH 4 - PROGRESS: at 39.31% examples, 82972 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:38,079 : INFO : EPOCH 4 - PROGRESS: at 40.90% examples, 82733 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:39,085 : INFO : EPOCH 4 - PROGRESS: at 42.52% examples, 83008 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:40,109 : INFO : EPOCH 4 - PROGRESS: at 44.01% examples, 82974 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:41,158 : INFO : EPOCH 4 - PROGRESS: at 45.73% examples, 83316 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:42,169 : INFO : EPOCH 4 - PROGRESS: at 47.23% examples, 83342 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:43,177 : INFO : EPOCH 4 - PROGRESS: at 48.68% examples, 83345 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:44,203 : INFO : EPOCH 4 - PROGRESS: at 50.30% examples, 83501 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:50:45,206 : INFO : EPOCH 4 - PROGRESS: at 51.92% examples, 83722 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:46,211 : INFO : EPOCH 4 - PROGRESS: at 53.40% examples, 83724 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:47,241 : INFO : EPOCH 4 - PROGRESS: at 55.06% examples, 83845 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:48,257 : INFO : EPOCH 4 - PROGRESS: at 56.48% examples, 83820 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:49,401 : INFO : EPOCH 4 - PROGRESS: at 58.04% examples, 83689 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:50,452 : INFO : EPOCH 4 - PROGRESS: at 59.54% examples, 83764 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:51,458 : INFO : EPOCH 4 - PROGRESS: at 61.03% examples, 83762 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:52,537 : INFO : EPOCH 4 - PROGRESS: at 62.64% examples, 83766 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:53,544 : INFO : EPOCH 4 - PROGRESS: at 64.08% examples, 83760 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:54,548 : INFO : EPOCH 4 - PROGRESS: at 65.63% examples, 83906 words/s, in_qsize 6, out_qsize 0\n",
      "2020-12-02 07:50:55,587 : INFO : EPOCH 4 - PROGRESS: at 67.13% examples, 83853 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:56,591 : INFO : EPOCH 4 - PROGRESS: at 68.77% examples, 84002 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:57,610 : INFO : EPOCH 4 - PROGRESS: at 70.33% examples, 84113 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:58,748 : INFO : EPOCH 4 - PROGRESS: at 71.95% examples, 84011 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:50:59,846 : INFO : EPOCH 4 - PROGRESS: at 73.47% examples, 83990 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:00,881 : INFO : EPOCH 4 - PROGRESS: at 75.24% examples, 84186 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:01,918 : INFO : EPOCH 4 - PROGRESS: at 76.86% examples, 84248 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:51:03,040 : INFO : EPOCH 4 - PROGRESS: at 78.46% examples, 84189 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:04,132 : INFO : EPOCH 4 - PROGRESS: at 80.14% examples, 84305 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:05,139 : INFO : EPOCH 4 - PROGRESS: at 81.79% examples, 84409 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:06,338 : INFO : EPOCH 4 - PROGRESS: at 83.47% examples, 84225 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:07,389 : INFO : EPOCH 4 - PROGRESS: at 85.01% examples, 84279 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:08,391 : INFO : EPOCH 4 - PROGRESS: at 86.59% examples, 84400 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:09,458 : INFO : EPOCH 4 - PROGRESS: at 88.14% examples, 84422 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:51:10,477 : INFO : EPOCH 4 - PROGRESS: at 89.75% examples, 84510 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:11,615 : INFO : EPOCH 4 - PROGRESS: at 91.25% examples, 84420 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:12,666 : INFO : EPOCH 4 - PROGRESS: at 92.97% examples, 84556 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:13,708 : INFO : EPOCH 4 - PROGRESS: at 94.62% examples, 84603 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:14,806 : INFO : EPOCH 4 - PROGRESS: at 96.27% examples, 84584 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:51:15,813 : INFO : EPOCH 4 - PROGRESS: at 97.89% examples, 84664 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:16,862 : INFO : EPOCH 4 - PROGRESS: at 99.37% examples, 84685 words/s, in_qsize 5, out_qsize 0\n",
      "2020-12-02 07:51:17,075 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-02 07:51:17,080 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-02 07:51:17,123 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-02 07:51:17,131 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-02 07:51:17,132 : INFO : EPOCH - 4 : training on 8034983 raw words (5720419 effective words) took 67.4s, 84844 effective words/s\n",
      "2020-12-02 07:51:18,304 : INFO : EPOCH 5 - PROGRESS: at 1.43% examples, 72451 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:19,307 : INFO : EPOCH 5 - PROGRESS: at 3.21% examples, 84423 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:20,333 : INFO : EPOCH 5 - PROGRESS: at 4.87% examples, 85580 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:21,426 : INFO : EPOCH 5 - PROGRESS: at 6.46% examples, 84935 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:51:22,548 : INFO : EPOCH 5 - PROGRESS: at 8.03% examples, 84226 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:23,555 : INFO : EPOCH 5 - PROGRESS: at 9.58% examples, 85041 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:24,584 : INFO : EPOCH 5 - PROGRESS: at 11.16% examples, 85512 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:25,593 : INFO : EPOCH 5 - PROGRESS: at 12.80% examples, 86043 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:26,634 : INFO : EPOCH 5 - PROGRESS: at 14.17% examples, 85403 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:27,704 : INFO : EPOCH 5 - PROGRESS: at 15.65% examples, 85273 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:28,855 : INFO : EPOCH 5 - PROGRESS: at 17.12% examples, 84720 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:29,861 : INFO : EPOCH 5 - PROGRESS: at 18.94% examples, 85691 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:30,915 : INFO : EPOCH 5 - PROGRESS: at 20.54% examples, 85755 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:32,024 : INFO : EPOCH 5 - PROGRESS: at 22.18% examples, 85521 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:33,065 : INFO : EPOCH 5 - PROGRESS: at 23.76% examples, 85601 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:34,107 : INFO : EPOCH 5 - PROGRESS: at 25.30% examples, 85710 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:35,224 : INFO : EPOCH 5 - PROGRESS: at 27.01% examples, 85766 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:36,284 : INFO : EPOCH 5 - PROGRESS: at 28.70% examples, 85770 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:37,338 : INFO : EPOCH 5 - PROGRESS: at 30.20% examples, 85767 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:38,359 : INFO : EPOCH 5 - PROGRESS: at 31.72% examples, 85866 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:39,433 : INFO : EPOCH 5 - PROGRESS: at 33.29% examples, 85796 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:40,456 : INFO : EPOCH 5 - PROGRESS: at 34.80% examples, 85621 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:41,462 : INFO : EPOCH 5 - PROGRESS: at 36.52% examples, 86107 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:42,478 : INFO : EPOCH 5 - PROGRESS: at 37.98% examples, 85969 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:43,684 : INFO : EPOCH 5 - PROGRESS: at 39.68% examples, 85770 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:44,731 : INFO : EPOCH 5 - PROGRESS: at 41.41% examples, 86027 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:45,768 : INFO : EPOCH 5 - PROGRESS: at 43.03% examples, 86104 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:46,883 : INFO : EPOCH 5 - PROGRESS: at 44.62% examples, 85930 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:47,941 : INFO : EPOCH 5 - PROGRESS: at 46.17% examples, 85952 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:51:48,959 : INFO : EPOCH 5 - PROGRESS: at 47.71% examples, 85864 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:50,060 : INFO : EPOCH 5 - PROGRESS: at 49.55% examples, 86168 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:51,135 : INFO : EPOCH 5 - PROGRESS: at 51.21% examples, 86132 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:52,254 : INFO : EPOCH 5 - PROGRESS: at 52.81% examples, 85999 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:53,319 : INFO : EPOCH 5 - PROGRESS: at 54.43% examples, 85998 words/s, in_qsize 7, out_qsize 1\n",
      "2020-12-02 07:51:54,342 : INFO : EPOCH 5 - PROGRESS: at 56.15% examples, 86259 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:55,547 : INFO : EPOCH 5 - PROGRESS: at 57.65% examples, 85909 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:56,566 : INFO : EPOCH 5 - PROGRESS: at 59.18% examples, 85996 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:57,626 : INFO : EPOCH 5 - PROGRESS: at 60.79% examples, 85997 words/s, in_qsize 8, out_qsize 1\n",
      "2020-12-02 07:51:58,817 : INFO : EPOCH 5 - PROGRESS: at 62.52% examples, 85886 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:51:59,849 : INFO : EPOCH 5 - PROGRESS: at 64.08% examples, 85943 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:00,859 : INFO : EPOCH 5 - PROGRESS: at 65.75% examples, 86196 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:01,964 : INFO : EPOCH 5 - PROGRESS: at 67.37% examples, 86113 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:02,966 : INFO : EPOCH 5 - PROGRESS: at 69.01% examples, 86216 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:04,136 : INFO : EPOCH 5 - PROGRESS: at 70.58% examples, 86010 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:05,206 : INFO : EPOCH 5 - PROGRESS: at 72.45% examples, 86286 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:06,211 : INFO : EPOCH 5 - PROGRESS: at 73.82% examples, 86231 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-02 07:52:07,400 : INFO : EPOCH 5 - PROGRESS: at 75.63% examples, 86115 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:08,454 : INFO : EPOCH 5 - PROGRESS: at 77.39% examples, 86256 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:09,505 : INFO : EPOCH 5 - PROGRESS: at 78.99% examples, 86265 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:10,526 : INFO : EPOCH 5 - PROGRESS: at 80.40% examples, 86195 words/s, in_qsize 6, out_qsize 1\n",
      "2020-12-02 07:52:11,554 : INFO : EPOCH 5 - PROGRESS: at 82.02% examples, 86230 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:12,776 : INFO : EPOCH 5 - PROGRESS: at 83.96% examples, 86219 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:13,809 : INFO : EPOCH 5 - PROGRESS: at 85.49% examples, 86282 words/s, in_qsize 7, out_qsize 1\n",
      "2020-12-02 07:52:14,838 : INFO : EPOCH 5 - PROGRESS: at 87.13% examples, 86434 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:16,017 : INFO : EPOCH 5 - PROGRESS: at 88.76% examples, 86255 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:17,096 : INFO : EPOCH 5 - PROGRESS: at 90.30% examples, 86226 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:18,200 : INFO : EPOCH 5 - PROGRESS: at 91.97% examples, 86267 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:19,270 : INFO : EPOCH 5 - PROGRESS: at 93.71% examples, 86350 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:20,279 : INFO : EPOCH 5 - PROGRESS: at 95.33% examples, 86420 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:21,342 : INFO : EPOCH 5 - PROGRESS: at 97.05% examples, 86402 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:22,368 : INFO : EPOCH 5 - PROGRESS: at 98.44% examples, 86323 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-02 07:52:23,099 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-02 07:52:23,165 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-02 07:52:23,227 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-02 07:52:23,245 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-02 07:52:23,246 : INFO : EPOCH - 5 : training on 8034983 raw words (5720536 effective words) took 66.1s, 86533 effective words/s\n",
      "2020-12-02 07:52:23,247 : INFO : training on a 40174915 raw words (28601665 effective words) took 339.0s, 84382 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 7s, sys: 1.87 s, total: 11min 9s\n",
      "Wall time: 5min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build word2vec model\n",
    "w2v_num_features = 300\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
    "                                   min_count=10, workers=4, iter=5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QUZHFsj8RaGo"
   },
   "outputs": [],
   "source": [
    "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zWfcUVixRaGr"
   },
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                                     num_features=w2v_num_features)\n",
    "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                                    num_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GMe1dCfRaGw",
    "outputId": "9af98654-a437-4699-9895-95b1c1fc8496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model:> Train features shape: (35000, 300)  Test features shape: (15000, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mSFQ9H0RaGy"
   },
   "source": [
    "## Modeling with deep neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oz9gtU9RaGz"
   },
   "source": [
    "### Building Deep neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "eCI0uEBIRaGz"
   },
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, input_shape=(num_input_features,)))\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(256))\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(256))\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(1))\n",
    "    dnn_model.add(Activation('sigmoid'))\n",
    "\n",
    "    dnn_model.compile(loss='binary_crossentropy', optimizer='adam',                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3cQpTH6FRaG1"
   },
   "outputs": [],
   "source": [
    "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBzbo-YlRaG4"
   },
   "source": [
    "### Visualize sample deep architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fuhzYagRaG5",
    "outputId": "7bc8b162-84bb-41d3-846f-ebb8d92a2be2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 351,489\n",
      "Trainable params: 351,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "w2v_dnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-COslmPRaG8"
   },
   "source": [
    "### Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kcisAskRaG8",
    "outputId": "9949134f-4604-47c7-a12c-70bcb603bbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "315/315 [==============================] - 1s 3ms/step - loss: 0.3295 - accuracy: 0.8613 - val_loss: 0.2996 - val_accuracy: 0.8774\n",
      "Epoch 2/10\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 0.2964 - accuracy: 0.8771 - val_loss: 0.2992 - val_accuracy: 0.8780\n",
      "Epoch 3/10\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 0.2874 - accuracy: 0.8810 - val_loss: 0.3052 - val_accuracy: 0.8800\n",
      "Epoch 4/10\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 0.2815 - accuracy: 0.8843 - val_loss: 0.3004 - val_accuracy: 0.8763\n",
      "Epoch 5/10\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 0.2726 - accuracy: 0.8867 - val_loss: 0.2969 - val_accuracy: 0.8817\n",
      "Epoch 6/10\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 0.2658 - accuracy: 0.8904 - val_loss: 0.2905 - val_accuracy: 0.8777\n",
      "Epoch 7/10\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 0.2578 - accuracy: 0.8938 - val_loss: 0.2928 - val_accuracy: 0.8826\n",
      "Epoch 8/10\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 0.2494 - accuracy: 0.8969 - val_loss: 0.2997 - val_accuracy: 0.8783\n",
      "Epoch 9/10\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 0.2429 - accuracy: 0.9000 - val_loss: 0.3032 - val_accuracy: 0.8737\n",
      "Epoch 10/10\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 0.2347 - accuracy: 0.9040 - val_loss: 0.3063 - val_accuracy: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3991b7fa90>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=10, batch_size=batch_size, \n",
    "            shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7C1gcc4iRaG-",
    "outputId": "ca14eb81-c0a2-4cf9-ae84-31d32f13a229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-bf19a67cc778>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-02 07:53:14,146 : WARNING : From <ipython-input-30-bf19a67cc778>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:289: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
    "predictions = le.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "2XHEZ-X9RaG_",
    "outputId": "82c3ee83-2c2c-4e63-ea49-e11e3fcbd682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.91      0.88      7490\n",
      "    positive       0.90      0.83      0.87      7510\n",
      "\n",
      "    accuracy                           0.87     15000\n",
      "   macro avg       0.87      0.87      0.87     15000\n",
      "weighted avg       0.87      0.87      0.87     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6817</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1243</td>\n",
       "      <td>6267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6817       673\n",
       "positive      1243      6267"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "B_poFApvZBk3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "11_NLP_Applications_Text_Classification_Machine_Learning_and_Basic_DNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
