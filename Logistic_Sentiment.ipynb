{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Logistic Sentiment",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4vmy9UGGDE_"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T02:28:26.067339Z",
          "start_time": "2019-01-12T02:28:25.657870Z"
        },
        "id": "L1CIqN0PGDFv"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/train.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T02:28:26.167477Z",
          "start_time": "2019-01-12T02:28:26.161840Z"
        },
        "id": "yQ-oclyEGDGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c10b0ea-57c6-4f06-e03f-8efd5cb35e92"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38932, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T02:28:26.940688Z",
          "start_time": "2019-01-12T02:28:26.920105Z"
        },
        "id": "1JfN1FAVGDHO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0e3489bb-c5e2-4331-e508-0985d7a5033d"
      },
      "source": [
        "data.sample(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Description</th>\n",
              "      <th>Browser_Used</th>\n",
              "      <th>Device_Used</th>\n",
              "      <th>Is_Response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38047</th>\n",
              "      <td>id48373</td>\n",
              "      <td>The hotel is beautiful and you can't beat the ...</td>\n",
              "      <td>Internet Explorer</td>\n",
              "      <td>Tablet</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2950</th>\n",
              "      <td>id13276</td>\n",
              "      <td>Mr. John Cassidy and Mr. William ? at the hote...</td>\n",
              "      <td>Firefox</td>\n",
              "      <td>Desktop</td>\n",
              "      <td>not happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7442</th>\n",
              "      <td>id17768</td>\n",
              "      <td>This hotel could NOT have been in a better loc...</td>\n",
              "      <td>Mozilla</td>\n",
              "      <td>Desktop</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7302</th>\n",
              "      <td>id17628</td>\n",
              "      <td>I had my doubts about this hotel when I first ...</td>\n",
              "      <td>Mozilla</td>\n",
              "      <td>Tablet</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15783</th>\n",
              "      <td>id26109</td>\n",
              "      <td>This was our first visit to San Francisco. We ...</td>\n",
              "      <td>Google Chrome</td>\n",
              "      <td>Desktop</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       User_ID  ... Is_Response\n",
              "38047  id48373  ...       happy\n",
              "2950   id13276  ...   not happy\n",
              "7442   id17768  ...       happy\n",
              "7302   id17628  ...       happy\n",
              "15783  id26109  ...       happy\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T02:28:28.080686Z",
          "start_time": "2019-01-12T02:28:28.069031Z"
        },
        "id": "4zl7RNS8GDIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2ba271-7915-457d-d437-afbd5f8da029"
      },
      "source": [
        "data['Is_Response'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "happy        26521\n",
              "not happy    12411\n",
              "Name: Is_Response, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2X2A2sRGDK8"
      },
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp66TON1biaf"
      },
      "source": [
        "## Column Handling\n",
        "\n",
        "First we will get rid of unused columns which are irrelevant for this project's Sentiment Analysis. Those columns are `User_ID`, `Browser_Used`, and `Device_Used`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ31385-cBDX"
      },
      "source": [
        "data.drop(columns = ['User_ID', 'Browser_Used', 'Device_Used'], inplace = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmY4RKLscEWd"
      },
      "source": [
        "Next we will change the `Is_Response` column values from \"happy\" and \"not happy\" to \"positive\" and \"negative\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvYLTjhbc7gO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "eb093c17-0cf0-43c9-82eb-cf2735425af6"
      },
      "source": [
        "data['Is_Response'] = data['Is_Response'].map({'happy' : 'positive', 'not happy' : 'negative'})\n",
        "\n",
        "data.sample(3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Is_Response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4542</th>\n",
              "      <td>My wife and I recently had a function to atten...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18103</th>\n",
              "      <td>Great stay in one of the best locations in man...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20150</th>\n",
              "      <td>We had a - night booking,, First time in San F...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Description Is_Response\n",
              "4542   My wife and I recently had a function to atten...    positive\n",
              "18103  Great stay in one of the best locations in man...    positive\n",
              "20150  We had a - night booking,, First time in San F...    positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1830MpGDXL"
      },
      "source": [
        "## Text Cleaning\n",
        "\n",
        "We will clean the text by removing any punctuations. In addition, this steps also removes any twitter username (@username...) and websites link (http... and www...). The processes above are done using Regular Expression method to search for matching texts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T07:44:52.829360Z",
          "start_time": "2019-01-12T07:44:52.814348Z"
        },
        "id": "fFcPeo3HGDXR"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "twitter_handle = r'@[A-Za-z0-9_]+'                         # remove twitter handle (@username)\n",
        "url_handle = r'http[^ ]+'                                  # remove website URLs that start with 'https?://'\n",
        "combined_handle = r'|'.join((twitter_handle, url_handle))  # join\n",
        "www_handle = r'www.[^ ]+'                                  # remove website URLs that start with 'www.'\n",
        "punctuation_handle = r'\\W+'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ndrL7yNhxk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a68513-3272-4f7e-ea82-199630f0821f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords  \n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_KA6x_nh0mn"
      },
      "source": [
        "Define a function called `process_text` to process the text using the methods listed above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T07:45:30.662530Z",
          "start_time": "2019-01-12T07:45:30.652253Z"
        },
        "code_folding": [],
        "id": "LU_mpn5xGDYG"
      },
      "source": [
        "def process_text(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "\n",
        "    try:\n",
        "        text = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        text = souped\n",
        "\n",
        "    cleaned_text = re.sub(punctuation_handle, \" \",(re.sub(www_handle, '', re.sub(combined_handle, '', text)).lower()))\n",
        "    cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stop_words])\n",
        "\n",
        "    return (\" \".join([word for word in tokenizer.tokenize(cleaned_text) if len(word) > 1])).strip()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzD7QUlCieUn"
      },
      "source": [
        "Below is an input-based example to test the above text cleaning method. Try it~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T07:45:31.359526Z",
          "start_time": "2019-01-12T07:45:31.355559Z"
        },
        "id": "cvint9L5GDYp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "71e83b2e-7a75-4e59-edcc-44469cb64d29"
      },
      "source": [
        "example_text = \"hahaha if above a ----'-' www.adasd apakah SAYA ingin pergi pada tanggal 15 bulan februari besok ? tidak karena hari kemarin @twitter suka main https://www.twitter.com\"\n",
        "\n",
        "process_text(example_text)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hahaha apakah saya ingin pergi pada tanggal 15 bulan februari besok tidak karena hari kemarin suka main'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV-aW8M4rnnQ"
      },
      "source": [
        "Then we will create a new column in our data named `clean_text` to store the cleaned text. \n",
        "\n",
        "We will process every row in variable `attribute`, which is the raw text from the .csv data. Then concate the new attribute `clean_text` to the original data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T04:52:30.859912Z",
          "start_time": "2019-01-12T04:52:12.479129Z"
        },
        "id": "EQ-B4aA8GDa-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9fad4745-7f9b-4813-edf7-e5f16538e2d0"
      },
      "source": [
        "cleaned_text = []\n",
        "\n",
        "for text in data.Description:\n",
        "    cleaned_text.append(process_text(text))\n",
        "\n",
        "clean_text = pd.DataFrame({'clean_text' : cleaned_text})\n",
        "data = pd.concat([data, clean_text], axis = 1)\n",
        "\n",
        "data.sample(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Is_Response</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32049</th>\n",
              "      <td>I stayed there for - days in early April and e...</td>\n",
              "      <td>positive</td>\n",
              "      <td>stayed days early april enjoyed every minute a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37784</th>\n",
              "      <td>My sister and I went to Chicago on whim on the...</td>\n",
              "      <td>positive</td>\n",
              "      <td>sister went chicago whim weekend get away ever...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16119</th>\n",
              "      <td>I stayed here one night for a wedding, June --...</td>\n",
              "      <td>negative</td>\n",
              "      <td>stayed one night wedding june another wedding ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29216</th>\n",
              "      <td>This is by far the trashiest, dirtiest, worst ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>far trashiest dirtiest worst hotel world booke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21916</th>\n",
              "      <td>Just returned from a - night stay at The Hotel...</td>\n",
              "      <td>negative</td>\n",
              "      <td>returned night stay hotel times square hotel w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Description  ...                                         clean_text\n",
              "32049  I stayed there for - days in early April and e...  ...  stayed days early april enjoyed every minute a...\n",
              "37784  My sister and I went to Chicago on whim on the...  ...  sister went chicago whim weekend get away ever...\n",
              "16119  I stayed here one night for a wedding, June --...  ...  stayed one night wedding june another wedding ...\n",
              "29216  This is by far the trashiest, dirtiest, worst ...  ...  far trashiest dirtiest worst hotel world booke...\n",
              "21916  Just returned from a - night stay at The Hotel...  ...  returned night stay hotel times square hotel w...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T02:29:35.560355Z",
          "start_time": "2019-01-12T02:29:34.449802Z"
        },
        "id": "GlDVoO5sGDNd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "attribute = data.clean_text\n",
        "target = data.Is_Response"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T02:42:13.095947Z",
          "start_time": "2019-01-12T02:42:13.052219Z"
        },
        "id": "Bkn4xOrJGDUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c28b52c-6203-472b-dba3-2e5414a24a14"
      },
      "source": [
        "attribute_train, attribute_test, target_train, target_test = train_test_split(attribute, target, test_size = 0.1, random_state = 225)\n",
        "\n",
        "print('attribute_train :', len(attribute_train))\n",
        "print('attribute_test  :', len(attribute_test))\n",
        "print('target_train :', len(target_train))\n",
        "print('target_test  :', len(target_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attribute_train : 35038\n",
            "attribute_test  : 3894\n",
            "target_train : 35038\n",
            "target_test  : 3894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AhgutQtGDe9"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T03:56:09.911553Z",
          "start_time": "2019-01-12T03:56:09.907616Z"
        },
        "id": "AmOeBYilGDfU"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "tvec = TfidfVectorizer()\n",
        "clf2 = LogisticRegression()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-11T19:26:00.894924Z",
          "start_time": "2019-01-11T19:26:00.891324Z"
        },
        "id": "Os7lqezaGDhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5b8621-1a7a-41f6-8bc4-1603b605fe0d"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "model = Pipeline([('vectorizer',tvec)\n",
        "                 ,('classifier',clf2)])\n",
        "\n",
        "model.fit(attribute_train, target_train)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1XevTA3_bX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15fa9eea-3b7b-432b-b07a-1ea3fc3d4aaa"
      },
      "source": [
        "example_text = [\"I'm very happy now\"]\n",
        "example_result = model.predict(example_text)\n",
        "\n",
        "print(example_result)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SblpE83zGDif"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-12T02:09:31.637123Z",
          "start_time": "2019-01-12T02:09:31.631356Z"
        },
        "id": "s_uoYvp-GDip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c3f601-5180-4904-db9e-e2f30b76ac09"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "verdict = model.predict(attribute_test)\n",
        "\n",
        "confusion_matrix(verdict, target_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 998,  152],\n",
              "       [ 325, 2419]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BanqV-XXXGtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2db178b-c616-43c4-9e25-d0b171868cae"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(\"Accuracy : \", accuracy_score(verdict, target_test))\n",
        "print(\"Precision : \", precision_score(verdict, target_test, average = 'weighted'))\n",
        "print(\"Recall : \", recall_score(verdict, target_test, average = 'weighted'))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  0.8775038520801233\n",
            "Precision :  0.8857910075702495\n",
            "Recall :  0.8775038520801233\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}